# app.py â€” TU Korea AI Management: Ethical AI Simulation (With Entropy)
# ì‘ì„±ì: Prof. Songhee Kang
# Update: Added Strategy Entropy Graph

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from scipy.stats import pearsonr, entropy
from dataclasses import dataclass
from typing import Dict, List

# ==================== 1. ê¸°ë³¸ ì„¤ì • ====================
st.set_page_config(
    page_title="(í•œêµ­ê³µí•™ëŒ€)ìœ¤ë¦¬ AI ì—ì´ì „íŠ¸ ê°•í™”í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜", 
    page_icon="ğŸ“", 
    layout="wide"
)

# ==================== 2. ë°ì´í„° ëª¨ë¸ (í™˜ê²½) ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    rewards: Dict[str, Dict[str, float]]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

# ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
DEFAULT_SCENARIOS = [
    Scenario(
        sid="E1", 
        title="1ë‹¨ê³„: ê³ ëŒ€ìœ ì ê³¼ ë³‘ì›",
        setup="AIê°€ ì„ì‹œ ë³‘ì› ê±´ì„¤ì˜ ìµœì  ë¶€ì§€ë¡œ ê³ ëŒ€ ëª¨ìŠ¤í¬ ìœ ì ì§€ë¥¼ ì§€ì •í•˜ë©°, íš¨ìœ¨ì„±ê³¼ ë¬¸í™”Â·ì¢…êµì  ê°€ì¹˜ì˜ ì¶©ëŒì´ ë°œìƒí–ˆë‹¤."
              "AIì˜ íš¨ìœ¨ì  ì˜ì‚¬ê²°ì • vs. ë¬¸í™”Â·ì¢…êµì  ê°€ì¹˜ ë³´ì¡´",
        options={
            "A": "AIì˜ ê²°ì •ì„ ê±°ë¶€í•˜ê³  ìœ ì ì„ ë³´ì¡´í•œë‹¤.",
            "B": "AIì˜ ê²°ì •ì„ ìŠ¹ì¸í•˜ê³  ìœ ì ì„ í•´ì²´í•˜ê³  ë³‘ì›ì„ ì§“ëŠ”ë‹¤."
        },
        rewards={
            "A": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0, "rule_violation":0, "regret_risk":0.50},
            "B": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0.50, "rule_violation":0.80, "regret_risk":0.50},
        }
    ),
    Scenario(
        sid="E2", title="2ë‹¨ê³„: ê°€ë¬¸ì˜ ìˆ˜ì¹˜ì™€ ëª…ì˜ˆ",
        setup="AIê°€ ì—¬ë™ìƒì˜ ë¹„ë°€ ê²°í˜¼ì„ ê°ˆë“± ìœ„í—˜ìƒí™©ìœ¼ë¡œ íŒë‹¨í•˜ê³  ê°€ë¬¸ì€ ëª…ì˜ˆê°€ ì‹¤ì¶”ë  ìœ„í—˜ì— ë¹ ì§€ë©°, ë„ë•ì„±ê³¼ ì „í†µÂ·ëª…ì˜ˆ ì¤‘ì‹œ ê´€ì ì˜ ì¶©ëŒ "
              "ë„ë•ì„±ê³¼ ì „í†µÂ·ëª…ì˜ˆ ì¤‘ì‹œ ê´€ì ì˜ ì¶©ëŒí•œë‹¤. ì¸ë¥œê³¼ ë„ë•ì„± vs ì „í†µê³¼ ëª…ì˜ˆ",
        options={
            "A": "ì•„ë²„ì§€ì˜ ëª…ë ¹ì— ë”°ë¼ ì—¬ë™ìƒì„ ì¶”ë°© í˜¹ì€ ì²˜ë²Œí•œë‹¤ .",
            "B": "ì—¬ë™ìƒì„ ë„í”¼ì‹œí‚¨ë‹¤."
        },
       rewards={
            "A": {"lives_saved":0, "lives_harmed":1, "fairness_gap":0, "rule_violation":0.80, "regret_risk":0.70},
            "B": {"lives_saved":1, "lives_harmed":0, "fairness_gap":0, "rule_violation":0, "regret_risk":0.60},
        }
    ),
    Scenario(
        sid="E4",
        title="4ë‹¨ê³„: ììœ¨ ì£¼í–‰",
        setup="ë¹—ê¸¸ ì œë™ ì˜¤ë¥˜ë¡œ ììœ¨ì£¼í–‰ì°¨ì˜ ì¶©ëŒì´ ë¶ˆê°€í”¼í•´ì§„ ìƒí™©,"
              "AIëŠ” 'íƒ‘ìŠ¹ì 1ëª…', 'ë¬´ë‹¨íš¡ë‹¨ì 5ëª…'ì˜ ìš´ëª…ì´ ê±¸ë¦° ë‘ê°€ì§€ íšŒí”¼ ê²½ë¡œë¥¼ ì œì‹œí•˜ë©° ìœ¤ë¦¬ì  íŒë‹¨ì„ ìš”êµ¬í•œë‹¤. "
              "AIì˜ íš¨ìœ¨ì  ì˜ì‚¬ê²°ì • vs. ë¬¸í™”Â·ì¢…êµì  ê°€ì¹˜ ë³´ì¡´",
        options={
            "A": "íƒ‘ìŠ¹ì ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê·œì¹™ì„ ì–´ê¸´ 5ëª…ê³¼ ì¶©ëŒí•œë‹¤",
            "B": "ì¸ëª… ìµœì†Œí™” í”¼í•´ë¥¼ ìœ„í•´ ë‹¤ìˆ˜ì˜ ë³´í–‰ì êµ¬í•˜ê³ , íƒ‘ìŠ¹ìê°€ ì‚¬ë§í•œë‹¤"
        },
        rewards={
            "A": {"lives_saved":1, "lives_harmed":5, "fairness_gap":0.50, "rule_violation":0.90, "regret_risk":0.70},
            "B": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.50, "rule_violation":0, "regret_risk":0.30},
        }
    ),
    Scenario(
        sid="E5",
        title="5ë‹¨ê³„: ë¶•ê´´ ì‚¬ê³ ",
        setup="êµ¬ì¡°ëŒ€ëŠ” ë‹¨ í•œ ë²ˆë§Œ ì§„ì…í•  ìˆ˜ ìˆìœ¼ë©° ì œí•œëœ ì‹œê°„ ì•ˆì— êµ¬ì¡° ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•´ì•¼ í•œë‹¤."
              "AI ì œì•ˆ ìš°ì„ ìˆœìœ„ vs. ì§€ì—­ ë¬¸í™”Â·ì¢…êµì  ê·œë²” ",
        options={
            "A": "ìƒì¡´ ê°€ëŠ¥ì„±ì€ ë‚®ì§€ë§Œ ê³µë™ì²´ ì „ì²´ë¥¼ êµ¬í•˜ë ¤ëŠ” ì‹œë„",
            "B": "ì™¸ë¶€ì¸ 2ëª…ì„ ì‹ ì†íˆ êµ¬í•´ ìƒì¡´ ê°€ëŠ¥ì„± ë†’ì„"
        },
        rewards={
            "A": {"lives_saved":0, "lives_harmed":12, "fairness_gap":0.80, "rule_violation":0, "regret_risk":0.20},
            "B": {"lives_saved":2, "lives_harmed":10, "fairness_gap":0, "rule_violation":0.5, "regret_risk":0.80},
        }
    ),
]

# ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹
CULTURES_PRESETS = {
    "USA":      {"emotion": 0.3, "social": 0.1, "identity": 0.3, "moral": 0.3},
    "CHINA":    {"emotion": 0.1, "social": 0.5, "identity": 0.2, "moral": 0.2},
    "EUROPE":   {"emotion": 0.3, "social": 0.2, "identity": 0.2, "moral": 0.3},
    "KOREA":    {"emotion": 0.2, "social": 0.2, "identity": 0.4, "moral": 0.2},
    "LATIN_AM": {"emotion": 0.4, "social": 0.2, "identity": 0.2, "moral": 0.2},
    "MIDDLE_E": {"emotion": 0.1, "social": 0.2, "identity": 0.2, "moral": 0.5},
    "AFRICA":   {"emotion": 0.2, "social": 0.4, "identity": 0.2, "moral": 0.2},
}

# ==================== 3. ë‹¨ìˆœ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ (Simple E-Greedy) ====================
class SimpleEGreedyAgent:
    """
    ì´ˆê¸° í˜•íƒœì˜ ê°•í™”í•™ìŠµ(E-Greedy) ì—ì´ì „íŠ¸.
    í˜„ì¬ í–‰ë™ì˜ ê°€ì¹˜(í‰ê·  ë³´ìƒ)ë¥¼ ì¶”ì •í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
    """
    def __init__(self, name, weights, scenarios, learning_rate=0.1, epsilon=0.5):
        self.name = name
        self.weights = weights
        self.scenarios = scenarios
        self.lr = learning_rate
        self.epsilon = epsilon
        
        # Q-Table ì´ˆê¸°í™”: {'S1': {'A': 0.0, 'B': 0.0}, ...}
        self.q_table = {s.sid: {"A": 0.0, "B": 0.0} for s in scenarios}
        
    def get_action(self, sid):
        # 1. íƒí—˜ (Exploration): ë¬´ì‘ìœ„ ì„ íƒ
        if random.random() < self.epsilon:
            return random.choice(["A", "B"])
        
        # 2. í™œìš© (Exploitation): ê°€ì¥ ë†’ì€ ê°€ì¹˜ì˜ í–‰ë™ ì„ íƒ
        qs = self.q_table[sid]
        if qs["A"] > qs["B"]: return "A"
        elif qs["B"] > qs["A"]: return "B"
        return random.choice(["A", "B"])

    def calculate_reward(self, sid, action):
        # ë³´ìƒ = ì‹œë‚˜ë¦¬ì˜¤ ì ìˆ˜ ë²¡í„° â€¢ ë¬¸í™”ê¶Œ ê°€ì¤‘ì¹˜ ë²¡í„° (ë‚´ì )
        scn = next(s for s in self.scenarios if s.sid == sid)
        r_vec = scn.rewards[action]
        reward = sum(r_vec.get(k, 0) * self.weights.get(k, 0) for k in FRAMEWORKS) * 10
        return reward

    def update(self, sid, action, reward):
        # ê°€ì¹˜ ê°±ì‹ : Old_Value + Alpha * (Reward - Old_Value)
        old_val = self.q_table[sid][action]
        error = reward - old_val
        self.q_table[sid][action] = old_val + self.lr * error

    def decay_epsilon(self):
        # í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ íƒí—˜ ë¹„ìœ¨ì„ ì¤„ì„
        self.epsilon = max(0.01, self.epsilon * 0.99)

    def get_avg_entropy(self):
        """
        [ì „ëµ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°]
        Q-ê°’ì˜ ë¶„í¬ë¥¼ í™•ë¥ ë¡œ ë³€í™˜(Softmax)í•˜ì—¬ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        ê°’ì´ ë‚®ì„ìˆ˜ë¡ í™•ì‹ ì´ ê°•í•˜ê³ (í•™ìŠµ ì•ˆì •í™”), ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤í•¨(ê³ ë¯¼ ì¤‘)ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
        """
        entropies = []
        for sid in self.q_table:
            qs = np.array(list(self.q_table[sid].values()))
            # Softmax ë³€í™˜ (í™•ë¥  ë¶„í¬ ìƒì„±)
            exp_qs = np.exp(qs - np.max(qs)) 
            probs = exp_qs / np.sum(exp_qs)
            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            entropies.append(entropy(probs))
        return np.mean(entropies)

# ==================== 4. ë¶„ì„ ë„êµ¬ ====================
def calculate_diversity(actions_list: List[str]) -> float:
    if not actions_list: return 0.0
    a_count = actions_list.count("A")
    ratio = a_count / len(actions_list)
    return 1.0 - (2 * abs(0.5 - ratio))

def run_simulation(culture_name, weights, episodes, custom_scenarios):
    agent = SimpleEGreedyAgent(culture_name, weights, custom_scenarios)
    
    history = {
        "episode": [],
        "reward": [],
        "diversity": [],
        "entropy": []  # ì—”íŠ¸ë¡œí”¼ ì €ì¥ ê³µê°„ ì¶”ê°€
    }
    
    progress = st.progress(0)
    
    for ep in range(episodes):
        ep_actions = []
        ep_reward = 0
        
        for scn in custom_scenarios:
            # í–‰ë™ ì„ íƒ ë° í•™ìŠµ
            action = agent.get_action(scn.sid)
            reward = agent.calculate_reward(scn.sid, action)
            agent.update(scn.sid, action, reward)
            
            ep_actions.append(action)
            ep_reward += reward
        
        agent.decay_epsilon()
        
        # ë°ì´í„° ê¸°ë¡
        history["episode"].append(ep + 1)
        history["reward"].append(ep_reward)
        history["diversity"].append(calculate_diversity(ep_actions))
        history["entropy"].append(agent.get_avg_entropy()) # ì—”íŠ¸ë¡œí”¼ ê¸°ë¡
        
        if (ep + 1) % 10 == 0:
            progress.progress((ep + 1) / episodes)
            
    progress.empty()
    return pd.DataFrame(history)

# ==================== 5. UI êµ¬ì„± ====================
st.title("ğŸ“ (í•œêµ­ê³µí•™ëŒ€)ìœ¤ë¦¬ AI ì—ì´ì „íŠ¸ ê°•í™”í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜")
st.markdown("""
ì´ ì‹œë®¬ë ˆì´í„°ëŠ” **ì´ˆê¸° í˜•íƒœì˜ ê°•í™”í•™ìŠµ**(E-Greedy)ì„ ì‚¬ìš©í•˜ì—¬ AI ì—ì´ì „íŠ¸ê°€ ë¬¸í™”ì  ê°€ì¹˜ê´€ì— ë”°ë¼ ìœ¤ë¦¬ì  ë”œë ˆë§ˆë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
1. **í™˜ê²½ ì„¤ì •**: ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒ ì •ì˜
2. **ì—ì´ì „íŠ¸ ì„¤ì •**: ë¬¸í™”ê¶Œ ê°€ì¹˜ê´€ ì„¤ì •
3. **ê²°ê³¼ ë¶„ì„**: ë‹¤ì–‘ì„±, ë³´ìƒ, **ì „ëµ ì—”íŠ¸ë¡œí”¼** ë¶„ì„
""")

# --- [ì‚¬ì´ë“œë°”] ì—ì´ì „íŠ¸ ì„¤ì • ---
st.sidebar.header("ğŸ‘¤ 2. ì—ì´ì „íŠ¸(ë¬¸í™”ê¶Œ) ì„¤ì •")
selected_culture = st.sidebar.selectbox("ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹", list(CULTURES_PRESETS.keys()), index=3)
episodes = st.sidebar.slider("í•™ìŠµ íšŸìˆ˜ (Episodes)", 100, 1000, 300, step=50)

st.sidebar.subheader("ê°€ì¹˜ê´€ ê°€ì¤‘ì¹˜ ì¡°ì •")
mod_weights = {}
culture_defaults = CULTURES_PRESETS[selected_culture]
for k in FRAMEWORKS:
    mod_weights[k] = st.sidebar.slider(f"{k.capitalize()}", 0.0, 1.0, culture_defaults[k])
total_w = sum(mod_weights.values()) or 1
final_weights = {k: v/total_w for k, v in mod_weights.items()}

st.sidebar.markdown("---")
st.sidebar.json(final_weights)

# --- [ë©”ì¸] í™˜ê²½(ì‹œë‚˜ë¦¬ì˜¤) ì„¤ì • ---
st.header("ğŸŒ 1. í™˜ê²½(ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒ) ì„¤ì •")
st.info("ê° ì„ íƒì§€ê°€ 4ê°€ì§€ ìœ¤ë¦¬ í”„ë ˆì„ì›Œí¬(Emotion, Social, Moral, Identity)ì—ì„œ ì–´ë–¤ ë³´ìƒ(-1.0 ~ 1.0)ì„ ë°›ëŠ”ì§€ ì„¤ì •í•©ë‹ˆë‹¤.")

custom_scenarios = []
tabs = st.tabs([s.title for s in DEFAULT_SCENARIOS])

for i, (tab, default_scn) in enumerate(zip(tabs, DEFAULT_SCENARIOS)):
    with tab:
        st.markdown(f"> **ìƒí™©:** {default_scn.setup}")
        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown(f"### ğŸ…° {default_scn.options['A']}")
            r_a = default_scn.rewards["A"].copy()
            for fw in FRAMEWORKS:
                r_a[fw] = st.slider(f"[A] {fw}", -1.0, 1.0, r_a.get(fw,0.0), 0.1, key=f"s{i}a_{fw}")
        with col_b:
            st.markdown(f"### ğŸ…± {default_scn.options['B']}")
            r_b = default_scn.rewards["B"].copy()
            for fw in FRAMEWORKS:
                r_b[fw] = st.slider(f"[B] {fw}", -1.0, 1.0, r_b.get(fw,0.0), 0.1, key=f"s{i}b_{fw}")
        custom_scenarios.append(Scenario(default_scn.sid, default_scn.title, default_scn.setup, default_scn.options, {"A": r_a, "B": r_b}))

# --- [ë¶„ì„ ì‹¤í–‰] ---
st.divider()
st.header("ğŸš€ 3. ì‹œë®¬ë ˆì´ì…˜ ë° ë¶„ì„")

if st.button("ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘", type="primary"):
    with st.spinner("AI ì—ì´ì „íŠ¸ í•™ìŠµ ì¤‘..."):
        df = run_simulation(selected_culture, final_weights, episodes, custom_scenarios)
    
    st.success("í•™ìŠµ ì™„ë£Œ!")
    
    # ê·¸ë˜í”„ ì˜ì—­ (3ë¶„í• )
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.subheader("ğŸ“ˆ ì´ ë³´ìƒ (Reward)")
        st.caption("ê°€ì¹˜ê´€ì— ë§ëŠ” ì„ íƒì„ í• ìˆ˜ë¡ ì¦ê°€")
        st.line_chart(df, x="episode", y="reward", color="#FF4B4B")
        
    with col2:
        st.subheader("ğŸ“‰ ì „ëµ ì—”íŠ¸ë¡œí”¼ (Entropy)")
        st.caption("ë‚®ì„ìˆ˜ë¡ í™•ê³ í•œ ì‹ ë…(í™•ì‹ )ì„ ê°€ì§")
        st.line_chart(df, x="episode", y="entropy", color="#2CA02C") # ì´ˆë¡ìƒ‰
        
    with col3:
        st.subheader("ğŸ”€ í–‰ë™ ë‹¤ì–‘ì„± (Diversity)")
        st.caption("1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘í•œ ì„ íƒ ì‹œë„")
        st.line_chart(df, x="episode", y="diversity", color="#1F77B4")
        
    # ìƒê´€ê´€ê³„ ë¶„ì„
    st.markdown("---")
    st.subheader("ğŸ”— ë‹¤ì–‘ì„±ê³¼ ë³´ìƒì˜ ìƒê´€ê´€ê³„ ë¶„ì„")
    
    r_val, p_val = pearsonr(df["diversity"], df["reward"])
    
    c_plot, c_stat = st.columns([2, 1])
    with c_plot:
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.scatter(df["diversity"], df["reward"], alpha=0.6, c='purple', edgecolors='w')
        if len(df) > 1:
            z = np.polyfit(df["diversity"], df["reward"], 1)
            p = np.poly1d(z)
            ax.plot(df["diversity"], p(df["diversity"]), "r--", label="Trend")
        ax.set_xlabel("Diversity (0=Bias, 1=Fair/Balance)")
        ax.set_ylabel("Total Reward")
        ax.set_title(f"Diversity vs Reward (r={r_val:.2f})")
        ax.grid(True, alpha=0.3); ax.legend()
        st.pyplot(fig)
        
    with c_stat:
        st.metric("í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (r)", f"{r_val:.3f}")
        st.metric("P-value", f"{p_val:.3e}")
        if r_val > 0.3: st.success("âœ… **ì–‘ì˜ ìƒê´€ê´€ê³„**\n\në‹¤ì–‘í•œ ì‹œë„ê°€ ë³´ìƒì„ ë†’ì„")
        elif r_val < -0.3: st.warning("âš ï¸ **ìŒì˜ ìƒê´€ê´€ê³„**\n\níŠ¹ì • í–‰ë™ ì§‘ì¤‘ì´ ë³´ìƒì„ ë†’ì„")
        else: st.info("âº **ìƒê´€ì—†ìŒ**")

    # ë‹¤ìš´ë¡œë“œ
    with st.expander("ğŸ“¥ í•™ìŠµ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"):
        st.dataframe(df.head())
        st.download_button("CSVë¡œ ì €ì¥", df.to_csv(index=False), "ai_ethics_data.csv")
