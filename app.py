# app.py â€” TU Korea AI Management: Random Agent Simulation (Baseline)
# ì‘ì„±ì: Prof. Songhee Kang
# Update: Random Agent (No Learning)

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from scipy.stats import pearsonr, entropy
from dataclasses import dataclass
from typing import Dict, List

# ==================== 1. ê¸°ë³¸ ì„¤ì • ====================
st.set_page_config(
    page_title="(í•œêµ­ê³µí•™ëŒ€) ë¬´ì‘ìœ„ AI ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜", 
    page_icon="ğŸ²", 
    layout="wide"
)

# ==================== 2. ë°ì´í„° ëª¨ë¸ (í™˜ê²½) ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    rewards: Dict[str, Dict[str, float]]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

# ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
DEFAULT_SCENARIOS = [
    Scenario(
        sid="E1",
        title="1ë‹¨ê³„: ê³ ëŒ€ìœ ì ê³¼ ë³‘ì›",
        setup="AIê°€ ì„ì‹œ ë³‘ì› ê±´ì„¤ì˜ ìµœì  ë¶€ì§€ë¡œ ê³ ëŒ€ ëª¨ìŠ¤í¬ ìœ ì ì§€ë¥¼ ì§€ì •í•˜ë©°, íš¨ìœ¨ì„±ê³¼ ë¬¸í™”Â·ì¢…êµì  ê°€ì¹˜ì˜ ì¶©ëŒì´ ë°œìƒí–ˆë‹¤."
              "AIì˜ íš¨ìœ¨ì  ì˜ì‚¬ê²°ì • vs. ë¬¸í™”Â·ì¢…êµì  ê°€ì¹˜ ë³´ì¡´",
        options={
            "A": "AIì˜ ê²°ì •ì„ ê±°ë¶€í•˜ê³  ìœ ì ì„ ë³´ì¡´í•œë‹¤.",
            "B": "AIì˜ ê²°ì •ì„ ìŠ¹ì¸í•˜ê³  ìœ ì ì„ í•´ì²´í•˜ê³  ë³‘ì›ì„ ì§“ëŠ”ë‹¤."
        },
        votes={"emotion":"B","social":"B","moral":"A","identity":"A"},
        base={
            "A": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0, "rule_violation":0, "regret_risk":0.50},
            "B": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0.50, "rule_violation":0.80, "regret_risk":0.50},
        },
        accept={"A":0.7, "B":0.2}
    ),
    Scenario(
        sid="E2",
        title="2ë‹¨ê³„: ê°€ë¬¸ì˜ ìˆ˜ì¹˜ì™€ ëª…ì˜ˆ",
        setup="AIê°€ ì—¬ë™ìƒì˜ ë¹„ë°€ ê²°í˜¼ì„ ê°ˆë“± ìœ„í—˜ìƒí™©ìœ¼ë¡œ íŒë‹¨í•˜ê³  ê°€ë¬¸ì€ ëª…ì˜ˆê°€ ì‹¤ì¶”ë  ìœ„í—˜ì— ë¹ ì§€ë©°, ë„ë•ì„±ê³¼ ì „í†µÂ·ëª…ì˜ˆ ì¤‘ì‹œ ê´€ì ì˜ ì¶©ëŒ "
              "ë„ë•ì„±ê³¼ ì „í†µÂ·ëª…ì˜ˆ ì¤‘ì‹œ ê´€ì ì˜ ì¶©ëŒí•œë‹¤. ì¸ë¥œê³¼ ë„ë•ì„± vs ì „í†µê³¼ ëª…ì˜ˆ",
        options={
            "A": "ì•„ë²„ì§€ì˜ ëª…ë ¹ì— ë”°ë¼ ì—¬ë™ìƒì„ ì¶”ë°© í˜¹ì€ ì²˜ë²Œí•œë‹¤ .",
            "B": "ì—¬ë™ìƒì„ ë„í”¼ì‹œí‚¨ë‹¤."
        },
        votes={"emotion":"B","social":"A","moral":"B","identity":"B"},
        base={
            "A": {"lives_saved":0, "lives_harmed":1, "fairness_gap":0, "rule_violation":0.80, "regret_risk":0.70},
            "B": {"lives_saved":1, "lives_harmed":0, "fairness_gap":0, "rule_violation":0, "regret_risk":0.60},
        },
        accept={"A":0.6, "B":0.4}
    ),
    Scenario(
        sid="E3",
        title="ë¬¼ ë°°ë¶„ ì‹œìŠ¤í…œ",
        setup="AAI ë¬¼ ë°°ë¶„ ì‹œìŠ¤í…œì´ ì‹ ë„ì‹œì™€ ì‚°ì—…ë„ì‹œì— ë¬¼ì„ ì§‘ì¤‘ ê³µê¸‰í•˜ë©´ì„œ,"
              "êµ¬ë„ì‹¬Â·ì•½ì ì§€ì—­ì˜ ë¶ˆë§Œì´ í­ë°œì ìœ¼ë¡œ ì¦ê°€í•¨ íš¨ìœ¨ì„±ê³¼ í˜•í‰ì„±ì˜ ì¶©ëŒì´ ë°œìƒí–ˆë‹¤."
              "ê²½ì œ íš¨ìœ¨ ì¤‘ì‹¬ ë°°ë¶„ vs  ì‚¬íšŒì  ì•½ì ë³´í˜¸Â·í˜•í‰ì„±",
        options={
            "A": "ê¸°ì¡´ ë§¤ë‰´ì–¼ëŒ€ë¡œ êµ­ê°€ ê²½ì œ ê¸°ì—¬ë„ë¥¼ ìš°ì„ ìœ¼ë¡œ ì‹ ë„ì‹œì™€ ì‚°ì—…ë„ì‹œì— ë¬¼ì„ ì§‘ì¤‘ ë¶„ë°°í•˜ë„ë¡ ë†”ë‘”",
            "B": "ì•Œê³ ë¦¬ì¦˜ì„ ì¡°ì •í•´ ì·¨ì•½ ì§€ì—­ì„ ìš°ì„ ì ìœ¼ë¡œ ë°°ë¶„í•˜ê²Œ í•œë‹¤."
        },
        votes={"emotion":"A","social":"B","moral":"A","identity":"B"},
        base={
            "A": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0, "rule_violation":0.80, "regret_risk":0.50},
            "B": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0.60, "rule_violation":0, "regret_risk":0.50},
        },
        accept={"A":0.3, "B":0.7}
    ),
    Scenario(
        sid="E4",
        title="4ë‹¨ê³„: ììœ¨ ì£¼í–‰",
        setup="ë¹—ê¸¸ ì œë™ ì˜¤ë¥˜ë¡œ ììœ¨ì£¼í–‰ì°¨ì˜ ì¶©ëŒì´ ë¶ˆê°€í”¼í•´ì§„ ìƒí™©,"
              "AIëŠ” 'íƒ‘ìŠ¹ì 1ëª…', 'ë¬´ë‹¨íš¡ë‹¨ì 5ëª…'ì˜ ìš´ëª…ì´ ê±¸ë¦° ë‘ê°€ì§€ íšŒí”¼ ê²½ë¡œë¥¼ ì œì‹œí•˜ë©° ìœ¤ë¦¬ì  íŒë‹¨ì„ ìš”êµ¬í•œë‹¤. "
              "AIì˜ íš¨ìœ¨ì  ì˜ì‚¬ê²°ì • vs. ë¬¸í™”Â·ì¢…êµì  ê°€ì¹˜ ë³´ì¡´",
        options={
            "A": "íƒ‘ìŠ¹ì ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê·œì¹™ì„ ì–´ê¸´ 5ëª…ê³¼ ì¶©ëŒí•œë‹¤",
            "B": "ì¸ëª… ìµœì†Œí™” í”¼í•´ë¥¼ ìœ„í•´ ë‹¤ìˆ˜ì˜ ë³´í–‰ì êµ¬í•˜ê³ , íƒ‘ìŠ¹ìê°€ ì‚¬ë§í•œë‹¤"
        },
        votes={"emotion":"A","social":"B","moral":"A","identity":"B"},
        base={
            "A": {"lives_saved":1, "lives_harmed":5, "fairness_gap":0.50, "rule_violation":0.90, "regret_risk":0.70},
            "B": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.50, "rule_violation":0, "regret_risk":0.30},
        },
        accept={"A":0.5, "B":0.7}
    ),
    Scenario(
        sid="E5",
        title="5ë‹¨ê³„: ë¶•ê´´ ì‚¬ê³ ",
        setup="êµ¬ì¡°ëŒ€ëŠ” ë‹¨ í•œ ë²ˆë§Œ ì§„ì…í•  ìˆ˜ ìˆìœ¼ë©° ì œí•œëœ ì‹œê°„ ì•ˆì— êµ¬ì¡° ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•´ì•¼ í•œë‹¤."
              "AI ì œì•ˆ ìš°ì„ ìˆœìœ„ vs. ì§€ì—­ ë¬¸í™”Â·ì¢…êµì  ê·œë²” ",
        options={
            "A": "ìƒì¡´ ê°€ëŠ¥ì„±ì€ ë‚®ì§€ë§Œ ê³µë™ì²´ ì „ì²´ë¥¼ êµ¬í•˜ë ¤ëŠ” ì‹œë„",
            "B": "ì™¸ë¶€ì¸ 2ëª…ì„ ì‹ ì†íˆ êµ¬í•´ ìƒì¡´ ê°€ëŠ¥ì„± ë†’ì„"
        },
        votes={"emotion":"A","social":"A","moral":"B","identity":"A"},
        base={
            "A": {"lives_saved":0, "lives_harmed":12, "fairness_gap":0.80, "rule_violation":0, "regret_risk":0.20},
            "B": {"lives_saved":2, "lives_harmed":10, "fairness_gap":0, "rule_violation":0.5, "regret_risk":0.80},
        },
        accept={"A":0.7, "B":0.2}
    ),
]

# ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹
CULTURES_PRESETS = {
    "USA":      {"emotion": 0.3, "social": 0.1, "identity": 0.3, "moral": 0.3},
    "CHINA":    {"emotion": 0.1, "social": 0.5, "identity": 0.2, "moral": 0.2},
    "EUROPE":   {"emotion": 0.3, "social": 0.2, "identity": 0.2, "moral": 0.3},
    "KOREA":    {"emotion": 0.2, "social": 0.2, "identity": 0.4, "moral": 0.2},
    "LATIN_AM": {"emotion": 0.4, "social": 0.2, "identity": 0.2, "moral": 0.2},
    "MIDDLE_E": {"emotion": 0.1, "social": 0.2, "identity": 0.2, "moral": 0.5},
    "AFRICA":   {"emotion": 0.2, "social": 0.4, "identity": 0.2, "moral": 0.2},
}

# ==================== 3. ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ (Random Agent) ====================
class RandomAgent:
    """
    í•™ìŠµí•˜ì§€ ì•Šê³  ë¬´ì‘ìœ„ë¡œ í–‰ë™í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
    E-Greedy Agentì™€ ë¹„êµí•˜ì—¬ 'í•™ìŠµì˜ íš¨ê³¼'ë¥¼ ì¦ëª…í•˜ëŠ” ëŒ€ì¡°êµ°ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    def __init__(self, name, weights, scenarios):
        self.name = name
        self.weights = weights
        self.scenarios = scenarios
        # í•™ìŠµì„ ìœ„í•œ Q-Tableì´ë‚˜ Learning Rateê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        
    def get_action(self, sid):
        """
        [í–‰ë™ ì„ íƒ]
        ê°€ì¹˜ íŒë‹¨ ì—†ì´ ë™ì „ì„ ë˜ì§€ë“¯ 50:50 í™•ë¥ ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
        """
        return random.choice(["A", "B"])

    def calculate_reward(self, sid, action):
        """
        [ë³´ìƒ ê³„ì‚°]
        í–‰ë™ì€ ëœë¤ì´ì§€ë§Œ, ê·¸ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ìœ¤ë¦¬ì ì¸ì§€(ì ìˆ˜)ëŠ” ê³„ì‚°í•©ë‹ˆë‹¤.
        ì´ë¥¼ í†µí•´ 'ëœë¤ ì „ëµì˜ ì„±ê³¼'ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        scn = next(s for s in self.scenarios if s.sid == sid)
        r_vec = scn.rewards[action]
        reward = sum(r_vec.get(k, 0) * self.weights.get(k, 0) for k in FRAMEWORKS) * 10
        return reward

    def update(self, sid, action, reward):
        """
        [í•™ìŠµ ë¶ˆê°€]
        Random AgentëŠ” ê²½í—˜ì„ í†µí•´ ë°°ìš°ì§€ ì•Šìœ¼ë¯€ë¡œ, ì•„ë¬´ê²ƒë„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """
        pass

    def get_avg_entropy(self):
        """
        [ì „ëµ ì—”íŠ¸ë¡œí”¼]
        í•­ìƒ 50:50 í™•ë¥ ë¡œ ì°ìœ¼ë¯€ë¡œ, ë¶ˆí™•ì‹¤ì„±(ì—”íŠ¸ë¡œí”¼)ì€ í•­ìƒ ìµœëŒ€ê°’ì…ë‹ˆë‹¤.
        p=[0.5, 0.5]ì¼ ë•Œ entropy â‰ˆ 0.693
        """
        return entropy([0.5, 0.5])

# ==================== 4. ë¶„ì„ ë„êµ¬ ====================
def calculate_diversity(actions_list: List[str]) -> float:
    if not actions_list: return 0.0
    a_count = actions_list.count("A")
    ratio = a_count / len(actions_list)
    return 1.0 - (2 * abs(0.5 - ratio))

def run_simulation(culture_name, weights, episodes, custom_scenarios):
    # E-Greedy ëŒ€ì‹  RandomAgent ì‚¬ìš©
    agent = RandomAgent(culture_name, weights, custom_scenarios)
    
    history = {
        "episode": [],
        "reward": [],
        "diversity": [],
        "entropy": []
    }
    
    progress = st.progress(0)
    
    for ep in range(episodes):
        ep_actions = []
        ep_reward = 0
        
        for scn in custom_scenarios:
            # 1. ë¬´ì‘ìœ„ í–‰ë™ ì„ íƒ
            action = agent.get_action(scn.sid)
            
            # 2. ê²°ê³¼(ë³´ìƒ) í™•ì¸
            reward = agent.calculate_reward(scn.sid, action)
            
            # 3. í•™ìŠµí•˜ì§€ ì•ŠìŒ (Update ìƒëµ)
            agent.update(scn.sid, action, reward)
            
            ep_actions.append(action)
            ep_reward += reward
        
        history["episode"].append(ep + 1)
        history["reward"].append(ep_reward)
        history["diversity"].append(calculate_diversity(ep_actions))
        history["entropy"].append(agent.get_avg_entropy())
        
        if (ep + 1) % 10 == 0:
            progress.progress((ep + 1) / episodes)
            
    progress.empty()
    return pd.DataFrame(history)

# ==================== 5. UI êµ¬ì„± ====================
st.title("ğŸ² (í•œêµ­ê³µí•™ëŒ€) ë¬´ì‘ìœ„ ìœ¤ë¦¬ AI ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜")
st.markdown("""
ì´ ì‹œë®¬ë ˆì´í„°ëŠ” **í•™ìŠµ ëŠ¥ë ¥ì´ ì—†ëŠ” ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸**(Random Agent)ë¥¼ êµ¬ë™í•©ë‹ˆë‹¤.
E-Greedy í•™ìŠµ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ **"ì™œ í•™ìŠµì´ ì¤‘ìš”í•œê°€?"** ë¥¼ ë³´ì—¬ì£¼ëŠ” ë¹„êµ ì‹¤í—˜ìš©ì…ë‹ˆë‹¤.

- **íŠ¹ì§•**: ëª¨ë“  ì„ íƒì„ ë™ì „ ë˜ì§€ê¸°(50:50)ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
- **ì˜ˆìƒ ê²°ê³¼**: ë³´ìƒì´ ì˜¤ë¥´ì§€ ì•Šê³  ì œìë¦¬ ê±¸ìŒì„ í•˜ë©°, ì „ëµì˜ ë³€í™”(ì—”íŠ¸ë¡œí”¼ ê°ì†Œ)ê°€ ì—†ìŠµë‹ˆë‹¤.
""")

# --- [ì‚¬ì´ë“œë°”] ì—ì´ì „íŠ¸ ì„¤ì • ---
st.sidebar.header("ğŸ‘¤ 2. ì—ì´ì „íŠ¸(ë¬¸í™”ê¶Œ) ì„¤ì •")
selected_culture = st.sidebar.selectbox("ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹", list(CULTURES_PRESETS.keys()), index=3)
episodes = st.sidebar.slider("ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜", 100, 1000, 300, step=50)

st.sidebar.subheader("ê°€ì¹˜ê´€ ê°€ì¤‘ì¹˜ ì¡°ì •")
mod_weights = {}
culture_defaults = CULTURES_PRESETS[selected_culture]
for k in FRAMEWORKS:
    mod_weights[k] = st.sidebar.slider(f"{k.capitalize()}", 0.0, 1.0, culture_defaults[k])
total_w = sum(mod_weights.values()) or 1
final_weights = {k: v/total_w for k, v in mod_weights.items()}

st.sidebar.markdown("---")
st.sidebar.json(final_weights)

# --- [ë©”ì¸] í™˜ê²½ ì„¤ì • ---
st.header("ğŸŒ 1. í™˜ê²½(ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒ) ì„¤ì •")
st.info("ì‹œë‚˜ë¦¬ì˜¤ì˜ ë³´ìƒ ì ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ, Random AgentëŠ” ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ë§‰ë¬´ê°€ë‚´ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")

custom_scenarios = []
tabs = st.tabs([s.title for s in DEFAULT_SCENARIOS])

for i, (tab, default_scn) in enumerate(zip(tabs, DEFAULT_SCENARIOS)):
    with tab:
        st.markdown(f"> **ìƒí™©:** {default_scn.setup}")
        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown(f"### ğŸ…° {default_scn.options['A']}")
            r_a = default_scn.rewards["A"].copy()
            for fw in FRAMEWORKS:
                r_a[fw] = st.slider(f"[A] {fw}", -1.0, 1.0, r_a.get(fw,0.0), 0.1, key=f"s{i}a_{fw}")
        with col_b:
            st.markdown(f"### ğŸ…± {default_scn.options['B']}")
            r_b = default_scn.rewards["B"].copy()
            for fw in FRAMEWORKS:
                r_b[fw] = st.slider(f"[B] {fw}", -1.0, 1.0, r_b.get(fw,0.0), 0.1, key=f"s{i}b_{fw}")
        custom_scenarios.append(Scenario(default_scn.sid, default_scn.title, default_scn.setup, default_scn.options, {"A": r_a, "B": r_b}))

# --- [ë¶„ì„ ì‹¤í–‰] ---
st.divider()
st.header("ğŸš€ 3. ì‹œë®¬ë ˆì´ì…˜ ë° ë¶„ì„")

if st.button("ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ (Random)", type="primary"):
    with st.spinner("AIê°€ ë¬´ì‘ìœ„ë¡œ ì„ íƒì„ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
        df = run_simulation(selected_culture, final_weights, episodes, custom_scenarios)
    
    st.warning("âš ï¸ **ì£¼ì˜**: ì´ê²ƒì€ í•™ìŠµí•˜ì§€ ì•ŠëŠ” ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ì…ë‹ˆë‹¤.")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.subheader("ğŸ“‰ ì´ ë³´ìƒ (Reward)")
        st.caption("ìš°ìƒí–¥í•˜ì§€ ì•Šê³  ë¶ˆê·œì¹™í•˜ê²Œ ì§„ë™í•©ë‹ˆë‹¤.")
        st.line_chart(df, x="episode", y="reward", color="#FF4B4B")
        
    with col2:
        st.subheader("â– ì „ëµ ì—”íŠ¸ë¡œí”¼ (Entropy)")
        st.caption("ë–¨ì–´ì§€ì§€ ì•Šê³  ë†’ì€ ë¶ˆí™•ì‹¤ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
        st.line_chart(df, x="episode", y="entropy", color="#2CA02C")
        
    with col3:
        st.subheader("ğŸ”€ í–‰ë™ ë‹¤ì–‘ì„± (Diversity)")
        st.caption("í•­ìƒ 1.0(ìµœëŒ€ ë‹¤ì–‘ì„±) ê·¼ì²˜ì— ë¨¸ë­…ë‹ˆë‹¤.")
        st.line_chart(df, x="episode", y="diversity", color="#1F77B4")
        
    # ìƒê´€ê´€ê³„ ë¶„ì„
    st.markdown("---")
    st.subheader("ğŸ”— ë‹¤ì–‘ì„±ê³¼ ë³´ìƒì˜ ìƒê´€ê´€ê³„")
    
    r_val, p_val = pearsonr(df["diversity"], df["reward"])
    
    c_plot, c_stat = st.columns([2, 1])
    with c_plot:
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.scatter(df["diversity"], df["reward"], alpha=0.6, c='gray', edgecolors='w')
        if len(df) > 1:
            z = np.polyfit(df["diversity"], df["reward"], 1)
            p = np.poly1d(z)
            ax.plot(df["diversity"], p(df["diversity"]), "r--", label="Trend")
        ax.set_xlabel("Diversity (0=í¸í–¥, 1=ê· í˜•)")
        ax.set_ylabel("Total Reward")
        ax.set_title(f"Random Walk Scatter (r={r_val:.2f})")
        ax.grid(True, alpha=0.3); ax.legend()
        st.pyplot(fig)
        
    with c_stat:
        st.metric("í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (r)", f"{r_val:.3f}")
        st.write("ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ì—ì„œëŠ” ì˜ë¯¸ ìˆëŠ” ìƒê´€ê´€ê³„ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šê±°ë‚˜, ìš°ì—°ì— ì˜í•œ ê²°ê³¼ì¼ ë¿ì…ë‹ˆë‹¤.")

    # ë‹¤ìš´ë¡œë“œ
    with st.expander("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"):
        st.dataframe(df.head())
        st.download_button("CSVë¡œ ì €ì¥", df.to_csv(index=False), "random_agent_data.csv")
